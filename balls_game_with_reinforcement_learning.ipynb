{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fefd564",
   "metadata": {},
   "source": [
    "#### inspiration: https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa5a04",
   "metadata": {},
   "source": [
    "# Implementation of the Q-learning reinforcement learning algorithm:\n",
    "## $\\epsilon$-greedy \n",
    "## for Balls game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8133322",
   "metadata": {},
   "source": [
    "# GAME MECHANICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016a20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from spider import *\n",
    "\n",
    "def createboard(n):\n",
    "    return np.ones((n, n), dtype=np.bool_)\n",
    "\n",
    "def number_to_array(number, n):\n",
    "    return np.array([*str(bin(number))[2:].rjust(n*n, \"0\")]).astype(np.bool_)\n",
    "\n",
    "def k_balls_combinations(k, n):\n",
    "    return np.sum(np.array(list(combinations(2**np.arange(n*n), k))), axis=1)\n",
    "\n",
    "def possible_moves(board, kernels, n):\n",
    "    moved_boards = set(np.sum(np.logical_and(kernels, board) * 2 ** np.arange(n**2-1, -1, -1), axis=1))\n",
    "    moved_boards.discard(np.sum(board * 2 ** np.arange(n**2-1, -1, -1)))\n",
    "    moved_boards.discard(0)\n",
    "    return moved_boards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77304cf7",
   "metadata": {},
   "source": [
    "# TYPES OF PLAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4344b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player_making_only_random_moves:\n",
    "    \n",
    "    def __init__(self, name, n):\n",
    "        self.name = name\n",
    "        self.wins = 0\n",
    "        self.losses = 0\n",
    "        self.kernels = np.bool_(spider_flat(board=createboard(n)))\n",
    "        self.n = n\n",
    "        \n",
    "    def make_move(self, board):\n",
    "        moves = possible_moves(board=board, kernels=self.kernels, n=self.n)\n",
    "        move = np.random.choice(list(moves))\n",
    "        return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc64308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player_which_learns:\n",
    "    \n",
    "    def __init__(self, name, n, epsilon):\n",
    "        self.name = name    \n",
    "        self.wins = 0\n",
    "        self.losses = 0\n",
    "        self.kernels = np.bool_(spider_flat(board=createboard(n)))\n",
    "        self.n = n\n",
    "        \n",
    "        ### Epsilon-greedy algorithm, Q-learning\n",
    "        # epsilon - a measure whether to choose exploration or exploitation\n",
    "        self.epsilon = epsilon        \n",
    "        # Q-values for certain actions\n",
    "        self.actions_values = np.zeros((2**n**2)-1)\n",
    "        # learning parameters\n",
    "        self.lr = 0.2\n",
    "        self.discount_factor = 0.9\n",
    "        \n",
    "    def make_move(self, board):\n",
    "        moves = list(possible_moves(board=board, kernels=self.kernels, n=self.n))\n",
    "        values_for_moves = self.actions_values[moves]\n",
    "        move_with_the_highest_value = moves[np.argmax(values_for_moves)]\n",
    "        \n",
    "        # 30 % exploration, 70 % exploitation\n",
    "        if np.random.uniform(0, 1) <= self.epsilon:\n",
    "            move = np.random.choice(moves)\n",
    "        else:\n",
    "            move = move_with_the_highest_value\n",
    "        return move\n",
    "    \n",
    "    def reward_learning(self, reward, actions):\n",
    "        for action in reversed(actions):\n",
    "            self.actions_values[action] += self.lr * (self.discount_factor * reward - self.actions_values[action])\n",
    "            reward = self.actions_values[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6054449",
   "metadata": {},
   "source": [
    "# Players initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07427941",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "\n",
    "player0 = Player_making_only_random_moves(name=0, n=n)\n",
    "player1 = Player_which_learns(name=1, n=n, epsilon=0.3)\n",
    "\n",
    "PLAYERS = [player0, player1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672a17c",
   "metadata": {},
   "source": [
    "# Simulation of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a472a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_start(n, who_starts):\n",
    "    board = createboard(n).reshape(n**2)\n",
    "    which_player_now = who_starts # 0 or 1\n",
    "    \n",
    "    player_wins = [False, False]    \n",
    "    players_moves = [[], []]\n",
    "\n",
    "    gameEnded = False\n",
    "    while not gameEnded:\n",
    "        \n",
    "        if np.sum(board) == 1:\n",
    "            player_wins[(which_player_now + 1) % 2] = True            \n",
    "            gameEnded = True\n",
    "            break\n",
    "        \n",
    "        # player makes its move\n",
    "        move = PLAYERS[which_player_now].make_move(board)\n",
    "        players_moves[which_player_now].append(move)\n",
    "        board = number_to_array(move, n)\n",
    "        \n",
    "        # player switches\n",
    "        which_player_now = (which_player_now + 1) % 2\n",
    "             \n",
    "    PLAYERS[(which_player_now + 1) % 2].wins += 1\n",
    "    PLAYERS[which_player_now].losses += 1    \n",
    "    \n",
    "    # LEARNING PROCESS according to the reward\n",
    "    # reward = 1 if player1 won, 0 if player1 lost\n",
    "    reward_for_player1 = player_wins.index(True)  \n",
    "    player1.reward_learning(reward_for_player1, players_moves[1])\n",
    "    # learning from the second player moves\n",
    "    player1.reward_learning(int(np.logical_not(reward_for_player1)), players_moves[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46375d3e",
   "metadata": {},
   "source": [
    "# Simulate N games\n",
    "### players start alternately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ac8255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d3863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 98.8688530921936 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "N = 100000\n",
    "for i in range(N):\n",
    "    game_start(n=4, who_starts=i%2)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6973b4",
   "metadata": {},
   "source": [
    "# The ratio of the winnings\n",
    "### for random moves player, and the one which learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12599f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1858, 0.8142)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player0.wins/(player0.wins+player0.losses), player1.wins/(player1.wins+player1.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81284d31",
   "metadata": {},
   "source": [
    "# Saving the learned policy for playing\n",
    "### Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31598a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_for_playing = player1.actions_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c3f9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff8edd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('action_values_4x4.pkl', 'wb') as fp:\n",
    "    pickle.dump(policy_for_playing, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a8a7c",
   "metadata": {},
   "source": [
    "# Loading the policy for playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4051f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('action_values_4x4.pkl', 'rb') as fp:\n",
    "    policy_for_playing = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc77d0",
   "metadata": {},
   "source": [
    "# New players initialization\n",
    "### and providing one of them  (now the 100%-greedy one) the policy to make choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51db7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "\n",
    "player0 = Player_making_only_random_moves(name=0, n=n)\n",
    "player1 = Player_which_learns(name=1, n=n, epsilon=0.0)\n",
    "\n",
    "PLAYERS = [player0, player1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2d58808",
   "metadata": {},
   "outputs": [],
   "source": [
    "player1.actions_values = policy_for_playing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e52bce",
   "metadata": {},
   "source": [
    "# Simulating the games again N times\n",
    "### to check the score of the learned policy used by greedy player against the random player\n",
    "### * player1 still learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "530254d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 91.32799410820007 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "N = 100000\n",
    "for i in range(N):\n",
    "    game_start(n=4, who_starts=i%2)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f0d1b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01569, 0.98431)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player0.wins/(player0.wins+player0.losses), player1.wins/(player1.wins+player1.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df688f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
